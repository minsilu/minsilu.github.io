

<!DOCTYPE html>
<html lang="en">
<head>
  


<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Text Sentimental Classification Based on the Twitter Dataset [ Minsi Lu ]</title>

  <link rel="icon" href="/img/my.ico">


    <meta name="author" content="Minsi Lu">





  <link rel="alternate" href="/atom.xml " title="Minsi Lu" type="application/atom+xml">


  
    <link rel="stylesheet" href="/css/main.css">
  


<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      processEscapes: true
    }
  });
  </script>
  
<meta name="generator" content="Hexo 7.1.1"></head>
  <body data-theme="light" class="notransition">
    <script>
      const body = document.body;
      const data = body.getAttribute("data-theme");
      const initTheme = (state) => {
        if (state === "dark") {
          body.setAttribute("data-theme", "dark");
        } else if (state === "light") {
          body.removeAttribute("data-theme");
        } else {
          localStorage.setItem("theme", data);
        }
      };
   
      initTheme(localStorage.getItem("theme"));
      
      setTimeout(() => body.classList.remove("notransition"), 75);
    </script>
  <div class="navbar" role="navigation">
    <nav class="menu">
      <input type="checkbox" id="menu-trigger" class="menu-trigger" />
      <label for="menu-trigger">
        <span class="menu-icon">
          <svg
            xmlns="http://www.w3.org/2000/svg"
            width="24"
            height="24"
            viewBox="0 0 512 512"
          >
            <path
              d="M64,384H448V341.33H64Zm0-106.67H448V234.67H64ZM64,128v42.67H448V128Z"
            />
          </svg>
        </span>
      </label>
      <a id="mode">
        <svg
          class="mode-sunny"
          xmlns="http://www.w3.org/2000/svg"
          width="24"
          height="24"
          viewBox="0 0 512 512"
        >
          <title>LIGHT</title>
          <line
            x1="256"
            y1="48"
            x2="256"
            y2="96"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="256"
            y1="416"
            x2="256"
            y2="464"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="403.08"
            y1="108.92"
            x2="369.14"
            y2="142.86"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="142.86"
            y1="369.14"
            x2="108.92"
            y2="403.08"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="464"
            y1="256"
            x2="416"
            y2="256"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="96"
            y1="256"
            x2="48"
            y2="256"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="403.08"
            y1="403.08"
            x2="369.14"
            y2="369.14"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="142.86"
            y1="142.86"
            x2="108.92"
            y2="108.92"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <circle
            cx="256"
            cy="256"
            r="80"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
        </svg>
        <svg
          class="mode-moon"
          xmlns="http://www.w3.org/2000/svg"
          width="24"
          height="24"
          viewBox="0 0 512 512"
        >
          <title>DARK</title>
          <line
            x1="256"
            y1="48"
            x2="256"
            y2="96"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="256"
            y1="416"
            x2="256"
            y2="464"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="403.08"
            y1="108.92"
            x2="369.14"
            y2="142.86"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="142.86"
            y1="369.14"
            x2="108.92"
            y2="403.08"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="464"
            y1="256"
            x2="416"
            y2="256"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="96"
            y1="256"
            x2="48"
            y2="256"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="403.08"
            y1="403.08"
            x2="369.14"
            y2="369.14"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <line
            x1="142.86"
            y1="142.86"
            x2="108.92"
            y2="108.92"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
          <circle
            cx="256"
            cy="256"
            r="80"
            style="stroke-linecap:round;stroke-miterlimit:10;stroke-width:32px"
          />
        </svg>
      </a>
      <div class="trigger">
        <div class="trigger-container">
          
            
            
            
            
            
            
            <a class="menu-link " href="/"> Home</a>
          
            
            
            
            
            
            
            <a class="menu-link " href="/about/"> About me</a>
          
            
            
            
            
            
            
            <a class="menu-link " href="/publications/"> Publications</a>
          
            
            
            
            
            
            
            <a class="menu-link " href="/projects/"> Projects</a>
          
            
            
            
            
            
            
            <a class="menu-link " href="/cv/mycv.pdf"> CV</a>
          
            
            
            
            
            
            
            <a class="menu-link " target="_blank" rel="noopener" href="https://github.com/minsilu"> GitHub</a>
          
            
            
            
            
            
            
            <a class="menu-link " href="/blog/"> Blog</a>
          
            
            
            
            
            
            
            <a class="menu-link " href="/contact/"> Contact</a>
          
        </div>
      </div>
    </nav>
  </div>
  
<div class="wrapper">
  <header class="header">
    <h1 class="header-title center">Text Sentimental Classification Based on the Twitter Dataset</h1>
  </header>
  <main class="page-content" aria-label="Content">
      <div class="post">
        <p>Repo: <a target="_blank" rel="noopener" href="https://github.com/minsilu/Text-Sentimental-Classification-based-on-Twitter-Dataset">Text Sentimental Classification Based on the Twitter Dataset</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This experiment aims to perform a four-way classification of text using a Twitter sentiment analysis dataset. Text data undergo a series of preprocessing steps to ensure optimal results. We trained five different models, including Decision Trees, Random Forest, Multilayer Perceptron (MLP), Residual Network (ResNet), and a BERT model with a classifier. These models were compared based on their performance on a validation set, with the BERT model chosen as the best due to its superior accuracy. The experiment ultimately showcased the performance of each model on the test set, with the BERT model with a classifier performing best in terms of accuracy.</p>
<h2 id="Data-Processing"><a href="#Data-Processing" class="headerlink" title="Data Processing"></a>Data Processing</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>The dataset used in this experiment is a Twitter sentiment analysis dataset. Given a piece of text and an entity, our task is to judge the sentiment of the text towards the entity. This dataset includes four categories: Positive, Negative, Neutral, and Irrelevant. We extract features from the text for subsequent classification. The dataset is large enough to train a variety of models and ensure the models’ generalizability. Each sentiment category is relatively balanced, avoiding the class imbalance issue that could bias the prediction model towards the majority class.</p>
<h3 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h3><p>To prepare the data for text analysis, we performed several preprocessing steps:</p>
<ul>
<li>Filtering: Filtering out non-alphanumeric characters to clean the data. These characters typically carry little meaning for sentiment analysis tasks and might interfere with the learning process.</li>
<li>Lowercasing: Converting all text to lowercase to avoid analysis complications and reduce duplicate features since text data is case-sensitive.</li>
</ul>
<h3 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h3><p>Since machine learning models cannot directly understand raw text data, tokenization is the first step in converting text into a numerical form. In this experiment, we used the nltk.word_tokenize module to split the cleaned text into individual word tokens.</p>
<h3 id="Removing-Stop-Words"><a href="#Removing-Stop-Words" class="headerlink" title="Removing Stop Words"></a>Removing Stop Words</h3><p>Stop words are common words (e.g., “is”, “the”, “and”) that usually don’t carry significant meaning and can be removed to reduce the dimensionality of the feature space. After tokenization, we used nltk.corpus.stopwords to identify and filter out these stop words.</p>
<h3 id="Feature-Extraction"><a href="#Feature-Extraction" class="headerlink" title="Feature Extraction"></a>Feature Extraction</h3><p>We manually implemented Word count feature extraction in this experiment. This method extracts each text into a vector, where the vector’s dimensions are the number of all tokens, and each dimension’s value corresponds to the count of the respective token in the text. Thus, this method results in a high-dimensional, sparse matrix, giving more weight to frequently occurring words.</p>
<p>However, for feature extraction used in subsequent model training, we adopted the Term Frequency-Inverse Document Frequency (TF-IDF) method. It considers not only the frequency of a word in a single document (Term Frequency) but also the inverse of the frequency at which the word appears across all documents (Inverse Document Frequency). This helps to balance the weight given to each word, assigning less weight to words that appear frequently across all documents and more weight to words that appear in specific documents only.</p>
<h3 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h3><p>For large text corpora, the TF-IDF process generates a high-dimensional feature space. In this experiment, the dimensionality produced by TF-IDF was 30,962. Although higher-dimensional vectors retain more information, they increase computational complexity and can lead to model overfitting. In practical training, we found that recommended dimensionality reduction techniques (e.g., hashing, random projection) might lose significant information. Therefore, we used Truncated Singular Value Decomposition (Truncated SVD) for dimensionality reduction in this experiment. Truncated SVD works directly on the data matrix, unlike PCA, which operates on the covariance matrix, making it more suitable for sparse matrices. After dimensionality reduction with Truncated SVD, the final feature dimensionality was reduced to 100.</p>
<h2 id="Model-Implementation"><a href="#Model-Implementation" class="headerlink" title="Model Implementation"></a>Model Implementation</h2><p>In this experiment, we utilized five different classifiers: Decision Tree (DT), Random Forest (RF), Multilayer Perceptron (MLPClassifier), Text-based Residual Network (TextResNetClassifier), and a Bert model with a classifier (BertClassifier). Decision Tree and Random Forest are directly employed from sklearn’s modules for classification. MLPClassifier, TextResNetClassifier, and BertClassifier, on the other hand, were specifically designed. Decision Tree (DT), Random Forest (RF), MLPClassifier, and TextResNetClassifier were trained on previously extracted features, while BertClassifier used the Bert pre-trained model for word embedding and was then trained with a classifier attached</p>
<h3 id="MLPClassifier"><a href="#MLPClassifier" class="headerlink" title="MLPClassifier"></a>MLPClassifier</h3><p>The MLPClassifier is implemented in the ManualModel class, following the sklearn-style interface, and is built using the PyTorch framework. This model consists of an input layer, several hidden layers, and an output layer, with all layers being fully connected. The hidden layers utilize the ReLU activation function. The model employs the Adam optimizer and cross-entropy loss function for training and incorporates techniques like learning rate decay and weight decay for optimization.</p>
<h3 id="BertClassifier"><a href="#BertClassifier" class="headerlink" title="BertClassifier"></a>BertClassifier</h3><p>The BertClassifier leverages the pre-trained BERT model from Hugging Face’s transformers library for text classification tasks. This model builds upon the BERT architecture by extracting embedding from input text using the pre-trained BERT model, followed by a dropout layer for regularization and a fully connected layer for classification. The output from the BERT model’s pooler is passed through the dropout and fully connected layer to obtain the final classification results. The model is trained using cross-entropy loss.</p>
<h3 id="TextResNetClassifier"><a href="#TextResNetClassifier" class="headerlink" title="TextResNetClassifier"></a>TextResNetClassifier</h3><p>The TextResNetClassifier is a text classification model based on convolutional neural networks. Inspired by the ResNet architecture, it utilizes residual connections to allow gradients to flow directly through the network. The architecture consists of one initial convolutional layer, eight residual blocks, one attention pooling layer, and a final fully connected layer for classification. Each residual block comprises two convolutional layers with batch normalization and dropout. The attention pooling layer in the TextResNetClassifier applies a learned attention mechanism to focus on the most significant parts of the text representation. The final classification is done using a fully connected layer.</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>In summary, the MLPClassifier is a straightforward yet effective baseline model. The BertClassifier leverages transfer learning from the pre-trained BERT model, while the TextResNetClassifier introduces convolutional and residual structures to text classification tasks. Designing these different models allows for a comprehensive comparison of various neural network architectures in text classification tasks.</p>
<h2 id="Parameter-Tuning"><a href="#Parameter-Tuning" class="headerlink" title="Parameter Tuning"></a>Parameter Tuning</h2><p>Model hyperparameters were adjusted through a series of trial and error experiments to optimize model performance on the validation set.</p>
<h3 id="BertClassifier-1"><a href="#BertClassifier-1" class="headerlink" title="BertClassifier"></a>BertClassifier</h3><p>In the case of Bert, different learning rates significantly impact training outcomes. Training results with different learning rates are shown in the figure below. A too high learning rate can cause the model to overshoot the minimum of the loss function, leading to diverging rather than converging loss values. This is why the loss value initially decreases rapidly (due to the high learning rate and large steps) and then rises to a plateau. Conversely, with a too low learning rate, the training process may fail to converge.</p>
<p><img src="/projects/twitter/f1.png" alt="Bert Performance at Different Learning Rates"></p>
<h3 id="TextResNetClassifier-1"><a href="#TextResNetClassifier-1" class="headerlink" title="TextResNetClassifier"></a>TextResNetClassifier</h3><p>Due to batch normalization technology, adjustments in learning rate did not significantly affect TextResNet’s final outcome during parameter tuning. However, unlike image classification, we found that using larger convolutional kernels yielded better results, as shown in the figure below. For text data, larger convolutional kernels can capture more contextual information in a single convolution operation. Thus, long-range contextual information may be more helpful for text classification tasks. Additionally, text data typically has lower dimensions than image data, allowing for the use of larger convolutional kernels without a drastic increase in the number of parameters.</p>
<p><img src="/projects/twitter/f2.png" alt="TextResNet Performance with Different Kernel Sizes"></p>
<h2 id="Performance-and-Computational-Efficiency"><a href="#Performance-and-Computational-Efficiency" class="headerlink" title="Performance and Computational Efficiency"></a>Performance and Computational Efficiency</h2><p>The table below shows the computational efficiency and performance of the three models, all trained on an NVIDIA A100 80GB PCIe:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Time&#x2F;Epoch</th>
<th>Total Training Time</th>
<th>GPU Memory Usage (MiB)</th>
<th>Hyperparameters</th>
</tr>
</thead>
<tbody><tr>
<td>BertClassifier</td>
<td>3 minutes</td>
<td>4 hours</td>
<td>5641</td>
<td>Max token length: 64, Batch size: 64, Epochs: 75, Learning rate: 2e-5, Dropout: 0.1</td>
</tr>
<tr>
<td>TextResNetClassifier</td>
<td>30 seconds</td>
<td>7-8 hours</td>
<td>1881</td>
<td>Batch size: 32, Learning rate: 0.001, Epochs: 890, Dropout: 0.5, Kernel size</td>
</tr>
</tbody></table>

       </div>
         <div class="dis">
    
     

  </div>
  </main>
</div>
  <footer class="footer">
    <small class="footer_copyright">
      <div id="bottom-inner">
        © 2021 Site by Minsi Lu.
      </div>
    </small>
  </footer>
  
  

    
      <script src="/js/main.js"></script>
    
      <script src="/js/custom-filter.js"></script>
    
  
  <script>
    window.FPConfig = {
      delay: 0,
      ignoreKeywords: [],
      maxRPS: 3,
      hoverDelay: 50,
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"></script>



  
</body>
</html>
